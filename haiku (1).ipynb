{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9525004,"sourceType":"datasetVersion","datasetId":5800006},{"sourceId":10154284,"sourceType":"datasetVersion","datasetId":6269147}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install necessary libraries\n!pip install torch torchvision torchaudio transformers\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertModel\nimport pandas as pd\nimport random\n\n# Load dataset\n# Make sure to upload your CSV file named 'haiku_dataset.csv' to Colab\ndf = pd.read_csv('/kaggle/input/final-dataset/final_dataset.csv')\n\n# Check the loaded data (optional)\nprint(df.head())\n\n# Define the custom Dataset class\nclass HaikuDataset(Dataset):\n    def __init__(self, questions, haikus, tokenizer, max_len=64):\n        self.questions = questions\n        self.haikus = haikus\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.questions)\n\n    def __getitem__(self, index):\n        question = str(self.questions[index])\n        haiku = str(self.haikus[index])\n\n        # Encode the question and haiku\n        input_encoding = self.tokenizer.encode_plus(\n            question,\n            haiku,\n            truncation=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': input_encoding['input_ids'].flatten(),\n            'attention_mask': input_encoding['attention_mask'].flatten(),\n            'labels': input_encoding['input_ids'].flatten()  # We will use the input_ids as labels for MLM\n        }\n\n# Define the Masking function\ndef mask_tokens(input_ids, mask_prob=0.15):\n    \"\"\"Mask tokens based on the given probability.\"\"\"\n    output = []\n    for ids in input_ids:\n        masked = []\n        for token in ids:\n            if random.random() < mask_prob:\n                # 80% of the time, replace with [MASK]\n                if random.random() < 0.8:\n                    masked.append(tokenizer.mask_token_id)  # Use the ID of the [MASK] token\n                # 10% of the time, keep the original token\n                elif random.random() < 0.5:\n                    masked.append(token)\n                # 10% of the time, replace with random token\n                else:\n                    masked.append(random.randint(0, tokenizer.vocab_size - 1))  # Random token ID\n            else:\n                masked.append(token)\n        output.append(masked)\n    return output\n\n# Define the Transformer Model\nclass TransformerModel(nn.Module):\n    def __init__(self, n_classes):\n        super(TransformerModel, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(self.bert.config.hidden_size, n_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        hidden_state = outputs[1]  # Get the pooled output\n        hidden_state = self.dropout(hidden_state)\n        logits = self.fc(hidden_state)\n        return logits\n\n# Hyperparameters\nBATCH_SIZE = 512\nEPOCHS = 3\nLEARNING_RATE = 2e-5\nMAX_LEN = 64\n\n# Load tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Create Dataset and DataLoader\ndataset = HaikuDataset(df['Question'].values, df['Haiku'].values, tokenizer, max_len=MAX_LEN)\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n# Initialize the model, optimizer, and loss function\nmodel = TransformerModel(n_classes=tokenizer.vocab_size)\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\nloss_fn = nn.CrossEntropyLoss()\n\n# Training Loop\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss = 0\n\n    for batch in dataloader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n\n        # Mask input ids\n        masked_input_ids = mask_tokens(input_ids.tolist())\n\n        # Convert masked tokens back to tensor\n        masked_input_ids_tensor = torch.tensor(masked_input_ids).to(device)\n\n        # Clear gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(masked_input_ids_tensor, attention_mask)\n        \n        # Calculate loss\n        loss = loss_fn(outputs.view(-1, tokenizer.vocab_size), input_ids.view(-1).to(device))\n        total_loss += loss.item()\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n\n    avg_loss = total_loss / len(dataloader)\n    print(f'Epoch {epoch + 1}/{EPOCHS}, Loss: {avg_loss:.4f}')\n\n# Save the trained model\ntorch.save(model.state_dict(), 'transformer_haiku_model.pth')\n\nprint(\"Training complete and model saved!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n\n# Load and preprocess data\ndef load_data(file_path):\n    df = pd.read_csv(file_path)\n    return df['Question'].tolist(), df['Haiku'].tolist()\n\ndef preprocess_data(questions, haikus):\n    all_text = ' '.join(questions + haikus)\n    chars = sorted(list(set(all_text)))\n    char_to_int = {c: i for i, c in enumerate(chars)}\n    int_to_char = {i: c for i, c in enumerate(chars)}\n    vocab_size = len(chars)\n    \n    encoded_questions = [[char_to_int[c] for c in q] for q in questions]\n    encoded_haikus = [[char_to_int[c] for c in h] for h in haikus]\n    \n    return encoded_questions, encoded_haikus, vocab_size, char_to_int, int_to_char\n\n# Custom Dataset\nclass QuestionHaikuDataset(Dataset):\n    def __init__(self, questions, haikus):\n        self.questions = questions\n        self.haikus = haikus\n\n    def __len__(self):\n        return len(self.questions)\n\n    def __getitem__(self, idx):\n        return (torch.tensor(self.questions[idx]), \n                torch.tensor(self.haikus[idx]),\n                len(self.questions[idx]),\n                len(self.haikus[idx]))\n\n# Collate function for padding sequences\ndef collate_fn(batch):\n    questions, haikus, q_lengths, h_lengths = zip(*batch)\n    questions_padded = pad_sequence(questions, batch_first=True, padding_value=0)\n    haikus_padded = pad_sequence(haikus, batch_first=True, padding_value=0)\n    return questions_padded, haikus_padded, torch.tensor(q_lengths), torch.tensor(h_lengths)\n\n# LSTM Model\nclass QuestionHaikuLSTM(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n        super(QuestionHaikuLSTM, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.encoder = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n        self.decoder = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, src, trg, src_lengths, teacher_forcing_ratio=0.5):\n        batch_size = src.size(0)\n        max_len = trg.size(1)\n        vocab_size = self.fc.out_features\n\n        outputs = torch.zeros(batch_size, max_len, vocab_size).to(src.device)\n        \n        embedded = self.embedding(src)\n        packed_embedded = pack_padded_sequence(embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False)\n        \n        _, hidden = self.encoder(packed_embedded)\n        \n        decoder_input = trg[:, 0]\n        \n        for t in range(1, max_len):\n            decoder_embedded = self.embedding(decoder_input).unsqueeze(1)\n            decoder_output, hidden = self.decoder(decoder_embedded, hidden)\n            prediction = self.fc(decoder_output.squeeze(1))\n            outputs[:, t] = prediction\n            \n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = prediction.max(1)[1]\n            decoder_input = trg[:, t] if teacher_force else top1\n\n        return outputs\n\n# Training function\ndef train(model, train_loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    for questions, haikus, q_lengths, h_lengths in train_loader:\n        questions, haikus = questions.to(device), haikus.to(device)\n        q_lengths, h_lengths = q_lengths.to(device), h_lengths.to(device)\n        \n        optimizer.zero_grad()\n        output = model(questions, haikus, q_lengths)\n        loss = criterion(output[:, 1:].reshape(-1, output.size(2)), haikus[:, 1:].reshape(-1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(train_loader)\n\n# Generate haiku\ndef generate_haiku(model, char_to_int, int_to_char, device, question, max_length=100):\n    model.eval()\n    with torch.no_grad():\n        input_seq = torch.tensor([char_to_int[c] for c in question]).unsqueeze(0).to(device)\n        input_length = torch.tensor([len(question)]).to(device)\n        \n        encoder_outputs, hidden = model.encoder(model.embedding(input_seq), input_length)\n        \n        decoder_input = torch.tensor([[char_to_int['\\n']]]).to(device)  # Start token\n        generated = []\n        \n        for _ in range(max_length):\n            decoder_output, hidden = model.decoder(model.embedding(decoder_input), hidden)\n            prediction = model.fc(decoder_output.squeeze(0))\n            \n            next_char_index = prediction.argmax(1).item()\n            if next_char_index == char_to_int.get('\\n', 0):  # Stop at newline\n                break\n            generated.append(int_to_char[next_char_index])\n            decoder_input = torch.tensor([[next_char_index]], device=device)\n            \n    return ''.join(generated)\n\n# Main function\ndef main():\n    # Hyperparameters\n    embed_size = 128\n    hidden_size = 256\n    num_layers = 2\n    batch_size = 64\n    num_epochs = 50\n    learning_rate = 0.001\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    file_path = '/kaggle/input/final-dataset/final_dataset.csv'  # Update this path\n    questions, haikus = load_data(file_path)\n    encoded_questions, encoded_haikus, vocab_size, char_to_int, int_to_char = preprocess_data(questions, haikus)\n\n    dataset = QuestionHaikuDataset(encoded_questions, encoded_haikus)\n    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n\n    model = QuestionHaikuLSTM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignore padding\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    for epoch in range(num_epochs):\n        loss = train(model, train_loader, criterion, optimizer, device)\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}')\n\n    torch.save(model.state_dict(), 'question_haiku_model.pth')\n\n    sample_question = \"What is AI?\"\n    generated_haiku = generate_haiku(model, char_to_int, int_to_char, device, sample_question)\n    print(f\"Question: {sample_question}\")\n    print(f\"Generated Haiku: {generated_haiku}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\n# Define your HaikuGenerator class (assuming it's already defined somewhere)\nclass HaikuGenerator(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers):\n        super(HaikuGenerator, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.encoder = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n        self.decoder = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, input_seq, input_length, hidden=None):\n        embedded = self.embedding(input_seq)\n        packed_input = pack_padded_sequence(embedded, input_length.cpu(), batch_first=True, enforce_sorted=False)\n        encoder_output, hidden = self.encoder(packed_input, hidden)\n        return encoder_output, hidden\n\ndef generate_haiku(model, char_to_int, int_to_char, device, question, max_length=100):\n    model.eval()\n    with torch.no_grad():\n        # Convert question to tensor\n        input_seq = torch.tensor([char_to_int[c] for c in question]).unsqueeze(0).to(device)\n        input_length = torch.tensor([len(question)]).to(device)\n\n        # Pack the input sequence\n        packed_input = pack_padded_sequence(model.embedding(input_seq), input_length.cpu(), batch_first=True, enforce_sorted=False)\n\n        # Encode the input\n        _, hidden = model.encoder(packed_input)\n\n        # Initialize decoder input as the start token (newline character)\n        decoder_input = torch.tensor([[char_to_int['\\n']]]).to(device)\n        generated = []\n\n        for _ in range(max_length):\n            decoder_embedded = model.embedding(decoder_input).unsqueeze(1)\n            decoder_output, hidden = model.decoder(decoder_embedded, hidden)\n            prediction = model.fc(decoder_output.squeeze(1))\n\n            next_char_index = prediction.argmax(1).item()\n            if next_char_index == char_to_int.get('\\n', 0):  # Stop at newline\n                break\n            generated.append(int_to_char[next_char_index])\n            decoder_input = torch.tensor([[next_char_index]], device=device)\n\n    return ''.join(generated)\n\ndef main():\n    # Assuming char_to_int and int_to_char are already defined\n    char_to_int = {'a': 0, 'b': 1, 'c': 2, '\\n': 3}  # Sample char_to_int, modify based on your vocabulary\n    int_to_char = {0: 'a', 1: 'b', 2: 'c', 3: '\\n'}  # Sample int_to_char, modify based on your vocabulary\n\n    # Hyperparameters\n    vocab_size = len(char_to_int)\n    embedding_dim = 128\n    hidden_size = 256\n    num_layers = 2\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Load model\n    model = HaikuGenerator(vocab_size, embedding_dim, hidden_size, num_layers).to(device)\n    model.load_state_dict(torch.load('/kaggle/working/question_haiku_model.pth'))\n    model.eval()\n\n    # Generate Haiku\n    sample_question = \"What is AI?\"\n    generated_haiku = generate_haiku(model, char_to_int, int_to_char, device, sample_question)\n    print(f\"Question: {sample_question}\")\n    print(f\"Generated Haiku: {generated_haiku}\")\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\n# Define your HaikuGenerator class\nclass HaikuGenerator(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers):\n        super(HaikuGenerator, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.encoder = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n        self.decoder = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, input_seq, input_length, hidden=None):\n        embedded = self.embedding(input_seq)\n        packed_input = pack_padded_sequence(embedded, input_length.cpu(), batch_first=True, enforce_sorted=False)\n        encoder_output, hidden = self.encoder(packed_input, hidden)\n        return encoder_output, hidden\n\ndef generate_haiku(model, char_to_int, int_to_char, device, question, max_length=100):\n    model.eval()\n    with torch.no_grad():\n        # Convert question to tensor\n        input_seq = torch.tensor([char_to_int[c] for c in question]).unsqueeze(0).to(device)\n        input_length = torch.tensor([len(question)]).to(device)\n\n        # Pack the input sequence\n        packed_input = pack_padded_sequence(model.embedding(input_seq), input_length.cpu(), batch_first=True, enforce_sorted=False)\n\n        # Encode the input\n        _, hidden = model.encoder(packed_input)\n\n        # Initialize decoder input as the start token (newline character)\n        decoder_input = torch.tensor([[char_to_int['\\n']]]).to(device)\n        generated = []\n\n        for _ in range(max_length):\n            decoder_embedded = model.embedding(decoder_input).unsqueeze(1)\n            decoder_output, hidden = model.decoder(decoder_embedded, hidden)\n            prediction = model.fc(decoder_output.squeeze(1))\n\n            next_char_index = prediction.argmax(1).item()\n            if next_char_index == char_to_int.get('\\n', 0):  # Stop at newline\n                break\n            generated.append(int_to_char[next_char_index])\n            decoder_input = torch.tensor([[next_char_index]], device=device)\n\n    return ''.join(generated)\n\ndef main():\n    # Load the original character mappings used during training\n    char_to_int = torch.load('char_to_int.pth')  # Load the original char_to_int\n    int_to_char = torch.load('int_to_char.pth')  # Load the original int_to_char\n\n    # Correct vocabulary size from the checkpoint\n    vocab_size = len(char_to_int)\n\n    # Hyperparameters\n    embedding_dim = 128\n    hidden_size = 256\n    num_layers = 2\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Load the model with correct vocab_size\n    model = HaikuGenerator(vocab_size, embedding_dim, hidden_size, num_layers).to(device)\n    model.load_state_dict(torch.load('/kaggle/working/question_haiku_model.pth'))\n    model.eval()\n\n    # Generate Haiku\n    sample_question = \"What is AI?\"\n    generated_haiku = generate_haiku(model, char_to_int, int_to_char, device, sample_question)\n    print(f\"Question: {sample_question}\")\n    print(f\"Generated Haiku: {generated_haiku}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence\nimport pandas as pd\n\n# Load and preprocess data\ndef load_data(file_path):\n    df = pd.read_csv(file_path)\n    return df['Question'].tolist(), df['Haiku'].tolist()\n\ndef preprocess_data(questions, haikus):\n    all_text = ' '.join(questions + haikus)\n    chars = sorted(list(set(all_text)))\n    char_to_int = {c: i for i, c in enumerate(chars)}\n    int_to_char = {i: c for i, c in enumerate(chars)}\n    vocab_size = len(chars)\n    \n    return vocab_size, char_to_int, int_to_char\n\n# Define your HaikuGenerator class\nclass HaikuGenerator(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers):\n        super(HaikuGenerator, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.encoder = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n        self.decoder = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, input_seq, input_length, hidden=None):\n        embedded = self.embedding(input_seq)\n        packed_input = pack_padded_sequence(embedded, input_length.cpu(), batch_first=True, enforce_sorted=False)\n        encoder_output, hidden = self.encoder(packed_input, hidden)\n        return encoder_output, hidden\n\ndef generate_haiku(model, char_to_int, int_to_char, device, question, max_length=100):\n    model.eval()\n    with torch.no_grad():\n        # Convert question to tensor\n        input_seq = torch.tensor([char_to_int[c] for c in question]).unsqueeze(0).to(device)\n        input_length = torch.tensor([len(question)]).to(device)\n\n        # Pack the input sequence\n        packed_input = pack_padded_sequence(model.embedding(input_seq), input_length.cpu(), batch_first=True, enforce_sorted=False)\n\n        # Encode the input\n        _, hidden = model.encoder(packed_input)\n\n        # Initialize decoder input as the start token (newline character)\n        decoder_input = torch.tensor([[char_to_int['\\n']]]).to(device)\n        generated = []\n\n        for _ in range(max_length):\n            decoder_embedded = model.embedding(decoder_input).unsqueeze(1)\n            decoder_output, hidden = model.decoder(decoder_embedded, hidden)\n            prediction = model.fc(decoder_output.squeeze(1))\n\n            next_char_index = prediction.argmax(1).item()\n            if next_char_index == char_to_int.get('\\n', 0):  # Stop at newline\n                break\n            generated.append(int_to_char[next_char_index])\n            decoder_input = torch.tensor([[next_char_index]], device=device)\n\n    return ''.join(generated)\n\ndef main():\n    # Load your dataset\n    file_path = '/kaggle/input/final-haiku-dataset/final_dataset.csv'  # Update this path\n    questions, haikus = load_data(file_path)\n\n    # Recreate the character mappings from the dataset\n    vocab_size, char_to_int, int_to_char = preprocess_data(questions, haikus)\n\n    # Save the mappings for future use\n    torch.save(char_to_int, 'char_to_int.pth')\n    torch.save(int_to_char, 'int_to_char.pth')\n\n    # Hyperparameters\n    embedding_dim = 128\n    hidden_size = 256\n    num_layers = 2\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Load the model with correct vocab_size\n    model = HaikuGenerator(vocab_size, embedding_dim, hidden_size, num_layers).to(device)\n    model.load_state_dict(torch.load('/kaggle/working/question_haiku_model.pth'))\n    model.eval()\n\n    # Generate Haiku\n    sample_question = \"What is AI?\"\n    generated_haiku = generate_haiku(model, char_to_int, int_to_char, device, sample_question)\n    print(f\"Question: {sample_question}\")\n    print(f\"Generated Haiku: {generated_haiku}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pandas as pd\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n# LSTM Model\nclass QuestionHaikuLSTM(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n        super(QuestionHaikuLSTM, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.encoder = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n        self.decoder = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n    def forward(self, src, trg, src_lengths, teacher_forcing_ratio=0.5):\n        # This method is for training, not needed during inference\n        pass\n# Function to load data and preprocess the vocab\ndef preprocess_data(questions, haikus):\n    all_text = ' '.join(questions + haikus)\n    chars = sorted(list(set(all_text)))\n    char_to_int = {c: i for i, c in enumerate(chars)}\n    int_to_char = {i: c for i, c in enumerate(chars)}\n    return char_to_int, int_to_char\n# Function to generate haiku based on a question\ndef generate_haiku(model, char_to_int, int_to_char, device, question, max_length=100):\n    model.eval()  # Set model to evaluation mode\n    with torch.no_grad():  # Disable gradient calculation for inference\n        # Convert question to tensor of indices\n        input_seq = torch.tensor([char_to_int.get(c, char_to_int[' ']) for c in question]).unsqueeze(0).to(device)\n        input_length = torch.tensor([len(question)]).to(device)\n        # Encode the input question\n        packed_embedded = pack_padded_sequence(model.embedding(input_seq), input_length.cpu(), batch_first=True, enforce_sorted=False)\n        _, hidden = model.encoder(packed_embedded)\n        # Decoder starts with the start token (newline character)\n        decoder_input = torch.tensor([[char_to_int['\\n']]]).to(device)\n        generated = []\n        for _ in range(max_length):\n            decoder_embedded = model.embedding(decoder_input).unsqueeze(1)\n            decoder_output, hidden = model.decoder(decoder_embedded, hidden)\n            prediction = model.fc(decoder_output.squeeze(1))\n            # Get the most likely next character\n            next_char_index = prediction.argmax(1).item()\n            if next_char_index == char_to_int.get('\\n', 0):  # Stop if newline character is generated\n                break\n            generated.append(int_to_char[next_char_index])\n            decoder_input = torch.tensor([[next_char_index]], device=device)\n    return ''.join(generated)\n# Main function for inference\ndef main():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    # Load vocabulary mappings from training\n    questions = [\"What is AI?\"]  # Dummy question for preprocessing the same vocab used during training\n    haikus = [\"This is a haiku\"]  # Dummy haiku for the same purpose\n    # Preprocess the data to get the vocab mappings\n    char_to_int, int_to_char = preprocess_data(questions, haikus)\n    # Define the model (the same architecture used during training)\n    vocab_size = len(char_to_int)\n    embed_size = 128\n    hidden_size = 256\n    num_layers = 2\n    model = QuestionHaikuLSTM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n    # Load the trained model weights\n    model.load_state_dict(torch.load('/kaggle/working/question_haiku_model.pth'))\n    # Input for inference\n    sample_question = \"What is AI?\"\n    # Generate the haiku\n    generated_haiku = generate_haiku(model, char_to_int, int_to_char, device, sample_question)\n    print(f\"Question: {sample_question}\")\n    print(f\"Generated Haiku: {generated_haiku}\")\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\n# Define your HaikuGenerator class (assuming it's already defined somewhere)\nclass HaikuGenerator(nn.Module):\n    def _init_(self, vocab_size, embedding_dim, hidden_size, num_layers):\n        super(HaikuGenerator, self)._init_()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.encoder = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n        self.decoder = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, input_seq, input_length, hidden=None):\n        embedded = self.embedding(input_seq)\n        packed_input = pack_padded_sequence(embedded, input_length.cpu(), batch_first=True, enforce_sorted=False)\n        encoder_output, hidden = self.encoder(packed_input, hidden)\n        return encoder_output, hidden\n\ndef generate_haiku(model, char_to_int, int_to_char, device, question, max_length=100):\n    model.eval()\n    with torch.no_grad():\n        # Convert question to tensor\n        input_seq = torch.tensor([char_to_int[c] for c in question]).unsqueeze(0).to(device)\n        input_length = torch.tensor([len(question)]).to(device)\n\n        # Pack the input sequence\n        packed_input = pack_padded_sequence(model.embedding(input_seq), input_length.cpu(), batch_first=True, enforce_sorted=False)\n\n        # Encode the input\n        _, hidden = model.encoder(packed_input)\n\n        # Initialize decoder input as the start token (newline character)\n        decoder_input = torch.tensor([[char_to_int['\\n']]]).to(device)\n        generated = []\n\n        for _ in range(max_length):\n            decoder_embedded = model.embedding(decoder_input).unsqueeze(1)\n            decoder_output, hidden = model.decoder(decoder_embedded, hidden)\n            prediction = model.fc(decoder_output.squeeze(1))\n\n            next_char_index = prediction.argmax(1).item()\n            if next_char_index == char_to_int.get('\\n', 0):  # Stop at newline\n                break\n            generated.append(int_to_char[next_char_index])\n            decoder_input = torch.tensor([[next_char_index]], device=device)\n\n    return ''.join(generated)\n\ndef main():\n    # Assuming char_to_int and int_to_char are already defined\n    char_to_int = {'a': 0, 'b': 1, 'c': 2, '\\n': 3}  # Sample char_to_int, modify based on your vocabulary\n    int_to_char = {0: 'a', 1: 'b', 2: 'c', 3: '\\n'}  # Sample int_to_char, modify based on your vocabulary\n\n    # Hyperparameters\n    vocab_size = len(char_to_int)\n    embedding_dim = 128\n    hidden_size = 256\n    num_layers = 2\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Load model\n    model = HaikuGenerator(vocab_size, embedding_dim, hidden_size, num_layers).to(device)\n    model.load_state_dict(torch.load('question_haiku_model.pth'))\n    model.eval()\n\n    # Generate Haiku\n    sample_question = \"What is AI?\"\n    generated_haiku = generate_haiku(model, char_to_int, int_to_char, device, sample_question)\n    print(f\"Question: {sample_question}\")\n    print(f\"Generated Haiku: {generated_haiku}\")\n\nif _name_ == “_main_”:\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n# Define the basic LSTM model\nclass BasicLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n        super(BasicLSTM, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        # LSTM layer\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        # Fully connected output layer\n        self.fc = nn.Linear(hidden_size, output_size)\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)  # Initial hidden state\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)  # Initial cell state\n        # Forward propagate LSTM\n        out, _ = self.lstm(x, (h0, c0))  # LSTM output and hidden states\n        # Pass the last time step's output through the fully connected layer\n        out = self.fc(out[:, -1, :])\n        return out\n# Create some dummy training data\ndef generate_dummy_data(sequence_length=10, num_samples=1000):\n    # Input will be random sequences of floats, target will be the sum of each sequence\n    X = torch.randn(num_samples, sequence_length, 1)  # Input: (batch_size, sequence_length, input_size)\n    y = X.sum(dim=1)  # Target: (batch_size, output_size), here output_size is 1 (sum of the sequence)\n    return X, y\n# Train the model\ndef train_model(model, X_train, y_train, num_epochs=10, learning_rate=0.001):\n    criterion = nn.MSELoss()  # Mean squared error for regression\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    model.train()\n    for epoch in range(num_epochs):\n        outputs = model(X_train)\n        loss = criterion(outputs, y_train)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if (epoch + 1) % 2 == 0:\n            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n# Run inference on new data\ndef run_inference(model, X_test):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(X_test)\n    return predictions\ndef main():\n    # Hyperparameters\n    input_size = 1\n    hidden_size = 50\n    output_size = 1\n    num_layers = 1\n    sequence_length = 10\n    num_epochs = 10\n    learning_rate = 0.001\n    # Generate dummy data\n    X_train, y_train = generate_dummy_data(sequence_length)\n    # Initialize model, loss, and optimizer\n    model = BasicLSTM(input_size, hidden_size, output_size, num_layers)\n    # Train the model\n    train_model(model, X_train, y_train, num_epochs, learning_rate)\n    # Generate new test data for inference\n    X_test, _ = generate_dummy_data(sequence_length, num_samples=5)  # 5 test samples\n    predictions = run_inference(model, X_test)\n    print(\"Test data predictions:\")\n    for i, prediction in enumerate(predictions):\n        print(f\"Sample {i + 1}: {prediction.item():.4f}\")\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\n# Dataset class for question-haiku pairs\nclass HaikuDataset(Dataset):\n    def __init__(self, questions, haikus, max_len):\n        self.questions = questions\n        self.haikus = haikus\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.questions)\n\n    def __getitem__(self, idx):\n        return self.questions[idx], self.haikus[idx]\n\n# Tokenizer and padding function\ndef tokenize_and_pad(texts, vocab, max_len):\n    tokenized = [[vocab.get(word, vocab['<UNK>']) for word in text.split()] for text in texts]\n    padded = [seq + [vocab['<PAD>']] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len] for seq in tokenized]\n    return np.array(padded)\n\n# Encoder class\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_dim, hidden_size, num_layers=1):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(input_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        _, (hidden, cell) = self.lstm(embedded)\n        return hidden, cell\n\n# Decoder class\nclass Decoder(nn.Module):\n    def __init__(self, output_size, embedding_dim, hidden_size, num_layers=1):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(output_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(1)\n        embedded = self.embedding(x)\n        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n        prediction = self.fc(output.squeeze(1))\n        return prediction, hidden, cell\n\n# Seq2Seq class (combining encoder and decoder)\nclass Seq2Seq(nn.Module):\n    def _init_(self, encoder, decoder, device):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, source, target, teacher_forcing_ratio=0.5):\n        batch_size = target.shape[0]\n        target_len = target.shape[1]\n        target_vocab_size = self.decoder.fc.out_features\n\n        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n\n        hidden, cell = self.encoder(source)\n        input = target[:, 0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(input, hidden, cell)\n            outputs[:, t] = output\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input = target[:, t] if teacher_force else top1\n\n        return outputs\n\n# Function to generate haiku from a question\ndef generate_haiku(model, question, vocab, idx_to_word, max_len=20):\n    model.eval()\n    with torch.no_grad():\n        question = torch.tensor(question).unsqueeze(0).to(model.device)\n        hidden, cell = model.encoder(question)\n        input = torch.tensor([vocab['<START>']]).to(model.device)\n        haiku = []\n\n        for _ in range(max_len):\n            output, hidden, cell = model.decoder(input, hidden, cell)\n            top1 = output.argmax(1)\n            word = idx_to_word[top1.item()]\n            if word == '<END>':\n                break\n            haiku.append(word)\n            input = top1\n\n        return ' '.join(haiku)\n\n# Training loop\ndef train_model(model, dataloader, optimizer, criterion, num_epochs):\n    model.train()\n    for epoch in range(num_epochs):\n        epoch_loss = 0\n        for questions, haikus in tqdm(dataloader):\n            questions, haikus = questions.to(model.device), haikus.to(model.device)\n            optimizer.zero_grad()\n            output = model(questions, haikus)\n            output_dim = output.shape[-1]\n            output = output[:, 1:].reshape(-1, output_dim)\n            haikus = haikus[:, 1:].reshape(-1)\n            loss = criterion(output, haikus)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n\n        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(dataloader):.4f}')\n\n# Function to load CSV data\ndef load_data_from_csv(filepath):\n    data = pd.read_csv(filepath)\n    questions = data['Question'].values\n    haikus = data['Haiku'].values\n    return questions, haikus\n\n# Main function to run the training and inference\ndef main():\n    # Load data from CSV\n    csv_file = '/kaggle/input/final-dataset/final_dataset.csv'  # Path to your CSV file\n    questions, haikus = load_data_from_csv(csv_file)\n\n    # Sample vocab and reverse mapping (should be adapted based on your data)\n    vocab = {'<PAD>': 0, '<START>': 1, '<END>': 2, '<UNK>': 3, 'What': 4, 'is': 5, 'the': 6, 'sky': 7, 'blue': 8, '...': 999}\n    idx_to_word = {idx: word for word, idx in vocab.items()}\n\n    # Hyperparameters\n    max_len = 10\n    embedding_dim = 256\n    hidden_size = 512\n    input_size = len(vocab)\n    output_size = len(vocab)\n    batch_size = 16\n    num_epochs = 20\n    learning_rate = 0.001\n\n    # Tokenize and pad questions and haikus\n    tokenized_questions = tokenize_and_pad(questions, vocab, max_len)\n    tokenized_haikus = tokenize_and_pad(haikus, vocab, max_len)\n\n    # Split the dataset into training and validation\n    train_questions, val_questions, train_haikus, val_haikus = train_test_split(tokenized_questions, tokenized_haikus, test_size=0.1)\n\n    # Dataset and DataLoader\n    train_dataset = HaikuDataset(train_questions, train_haikus, max_len)\n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n    # Initialize encoder, decoder, Seq2Seq model\n    encoder = Encoder(input_size, embedding_dim, hidden_size).to('cuda' if torch.cuda.is_available() else 'cpu')\n    decoder = Decoder(output_size, embedding_dim, hidden_size).to('cuda' if torch.cuda.is_available() else 'cpu')\n    model = Seq2Seq(encoder, decoder, 'cuda' if torch.cuda.is_available() else 'cpu')\n\n    # Optimizer and loss function\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    criterion = nn.CrossEntropyLoss(ignore_index=vocab['<PAD>'])\n\n    # Train the model\n    train_model(model, train_dataloader, optimizer, criterion, num_epochs)\n\n    # Test the model with an example\n    test_question = tokenize_and_pad([\"What is the sky\"], vocab, max_len)[0]\n    generated_haiku = generate_haiku(model, test_question, vocab, idx_to_word)\n    print(f\"Generated Haiku: {generated_haiku}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nclass HaikuDataset(Dataset):\n    def __init__(self, questions, haikus):\n        self.questions = questions\n        self.haikus = haikus\n    def __len__(self):\n        return len(self.questions)\n    def __getitem__(self, idx):\n        return self.questions[idx], self.haikus[idx]\ndef tokenize_and_pad(texts, vocab, max_len):\n    tokenized = [[vocab.get(word, vocab['<UNK>']) for word in text.split()] for text in texts]\n    padded = [seq + [vocab['<PAD>']] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len] for seq in tokenized]\n    return torch.tensor(padded, dtype=torch.long)\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_dim, hidden_size, num_layers=1):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(input_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n    def forward(self, x):\n        embedded = self.embedding(x)\n        _, (hidden, cell) = self.lstm(embedded)\n        return hidden, cell\nclass Decoder(nn.Module):\n    def __init__(self, output_size, embedding_dim, hidden_size, num_layers=1):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(output_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(1)\n        embedded = self.embedding(x)\n        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n        prediction = self.fc(output.squeeze(1))\n        return prediction, hidden, cell\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n    def forward(self, source, target, teacher_forcing_ratio=0.5):\n        batch_size = target.shape[0]\n        target_len = target.shape[1]\n        target_vocab_size = self.decoder.fc.out_features\n        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n        hidden, cell = self.encoder(source)\n        input = target[:, 0]\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(input, hidden, cell)\n            outputs[:, t] = output\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input = target[:, t] if teacher_force else top1\n        return outputs\ndef generate_haiku(model, question, vocab, idx_to_word, max_len=20):\n    model.eval()\n    with torch.no_grad():\n        question = question.unsqueeze(0).to(model.device)\n        hidden, cell = model.encoder(question)\n        input = torch.tensor([vocab['<START>']]).to(model.device)\n        haiku = []\n        for _ in range(max_len):\n            output, hidden, cell = model.decoder(input, hidden, cell)\n            top1 = output.argmax(1)\n            word = idx_to_word[top1.item()]\n            if word == '<END>':\n                break\n            haiku.append(word)\n            input = top1\n        return ' '.join(haiku)\ndef train_model(model, dataloader, optimizer, criterion, num_epochs):\n    model.train()\n    for epoch in range(num_epochs):\n        epoch_loss = 0\n        for questions, haikus in tqdm(dataloader):\n            questions, haikus = questions.to(model.device), haikus.to(model.device)\n            optimizer.zero_grad()\n            output = model(questions, haikus)\n            output_dim = output.shape[-1]\n            output = output[:, 1:].reshape(-1, output_dim)\n            haikus = haikus[:, 1:].reshape(-1)\n            loss = criterion(output, haikus)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(dataloader):.4f}')\ndef load_data_from_csv(filepath):\n    data = pd.read_csv(filepath)\n    questions = data['Question'].values\n    haikus = data['Haiku'].values\n    return questions, haikus\ndef build_vocab(texts, min_freq=2):\n    word_freq = {}\n    for text in texts:\n        for word in text.split():\n            word_freq[word] = word_freq.get(word, 0) + 1\n    vocab = {'<PAD>': 0, '<START>': 1, '<END>': 2, '<UNK>': 3}\n    for word, freq in word_freq.items():\n        if freq >= min_freq:\n            vocab[word] = len(vocab)\n    return vocab\ndef main():\n    # Load data from CSV\n    csv_file = '/kaggle/input/final-dataset/final_dataset.csv'\n    questions, haikus = load_data_from_csv(csv_file)\n    # Build vocabulary\n    all_texts = np.concatenate([questions, haikus])\n    vocab = build_vocab(all_texts)\n    idx_to_word = {idx: word for word, idx in vocab.items()}\n    # Hyperparameters\n    max_len = 50  # Increased max_len to accommodate longer sequences\n    embedding_dim = 256\n    hidden_size = 512\n    input_size = len(vocab)\n    output_size = len(vocab)\n    batch_size = 64  # Increased batch size\n    num_epochs = 50  # Increased number of epochs\n    learning_rate = 0.001\n    # Tokenize and pad questions and haikus\n    tokenized_questions = tokenize_and_pad(questions, vocab, max_len)\n    tokenized_haikus = tokenize_and_pad(haikus, vocab, max_len)\n    # Split the dataset into training and validation\n    train_questions, val_questions, train_haikus, val_haikus = train_test_split(tokenized_questions, tokenized_haikus, test_size=0.1)\n    # Dataset and DataLoader\n    train_dataset = HaikuDataset(train_questions, train_haikus)\n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    # Initialize encoder, decoder, Seq2Seq model\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    encoder = Encoder(input_size, embedding_dim, hidden_size).to(device)\n    decoder = Decoder(output_size, embedding_dim, hidden_size).to(device)\n    model = Seq2Seq(encoder, decoder, device).to(device)\n    # Optimizer and loss function\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    criterion = nn.CrossEntropyLoss(ignore_index=vocab['<PAD>'])\n    # Train the model\n    train_model(model, train_dataloader, optimizer, criterion, num_epochs)\n    # Test the model with an example\n    test_question = tokenize_and_pad([\"What is the sky\"], vocab, max_len)[0]\n    generated_haiku = generate_haiku(model, test_question, vocab, idx_to_word)\n    print(f\"Generated Haiku: {generated_haiku}\")\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_question = tokenize_and_pad([\"What is AI\"], vocab, max_len)[0]\ngenerated_haiku = generate_haiku(model, test_question, vocab, idx_to_word)\nprint(f\"Generated Haiku: {generated_haiku}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nclass HaikuDataset(Dataset):\n    def __init__(self, questions, haikus):\n        self.questions = questions\n        self.haikus = haikus\n    def __len__(self):\n        return len(self.questions)\n    def __getitem__(self, idx):\n        return self.questions[idx], self.haikus[idx]\ndef tokenize_and_pad(texts, vocab, max_len):\n    tokenized = [[vocab.get(word, vocab['<UNK>']) for word in text.split()] for text in texts]\n    padded = [seq + [vocab['<PAD>']] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len] for seq in tokenized]\n    return torch.tensor(padded, dtype=torch.long)\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_dim, hidden_size, num_layers=1):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(input_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n    def forward(self, x):\n        embedded = self.embedding(x)\n        _, (hidden, cell) = self.lstm(embedded)\n        return hidden, cell\nclass Decoder(nn.Module):\n    def __init__(self, output_size, embedding_dim, hidden_size, num_layers=1):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(output_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(1)\n        embedded = self.embedding(x)\n        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n        prediction = self.fc(output.squeeze(1))\n        return prediction, hidden, cell\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n    def forward(self, source, target, teacher_forcing_ratio=0.5):\n        batch_size = target.shape[0]\n        target_len = target.shape[1]\n        target_vocab_size = self.decoder.fc.out_features\n        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n        hidden, cell = self.encoder(source)\n        input = target[:, 0]\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(input, hidden, cell)\n            outputs[:, t] = output\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input = target[:, t] if teacher_force else top1\n        return outputs\ndef generate_haiku(model, question, vocab, idx_to_word, max_len=20):\n    model.eval()\n    with torch.no_grad():\n        question = question.unsqueeze(0).to(model.device)\n        hidden, cell = model.encoder(question)\n        input = torch.tensor([vocab['<START>']]).to(model.device)\n        haiku = []\n        for _ in range(max_len):\n            output, hidden, cell = model.decoder(input, hidden, cell)\n            top1 = output.argmax(1)\n            word = idx_to_word[top1.item()]\n            if word == '<END>':\n                break\n            haiku.append(word)\n            input = top1\n        return ' '.join(haiku)\ndef train_model(model, dataloader, optimizer, criterion, num_epochs):\n    model.train()\n    for epoch in range(num_epochs):\n        epoch_loss = 0\n        for questions, haikus in tqdm(dataloader):\n            questions, haikus = questions.to(model.device), haikus.to(model.device)\n            optimizer.zero_grad()\n            output = model(questions, haikus)\n            output_dim = output.shape[-1]\n            output = output[:, 1:].reshape(-1, output_dim)\n            haikus = haikus[:, 1:].reshape(-1)\n            loss = criterion(output, haikus)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(dataloader):.4f}')\ndef load_data_from_csv(filepath):\n    data = pd.read_csv(filepath)\n    questions = data['Question'].values\n    haikus = data['Haiku'].values\n    return questions, haikus\ndef build_vocab(texts, min_freq=2):\n    word_freq = {}\n    for text in texts:\n        for word in text.split():\n            word_freq[word] = word_freq.get(word, 0) + 1\n    vocab = {'<PAD>': 0, '<START>': 1, '<END>': 2, '<UNK>': 3}\n    for word, freq in word_freq.items():\n        if freq >= min_freq:\n            vocab[word] = len(vocab)\n    return vocab\ndef save_model(model, vocab, filepath):\n    \"\"\"\n    Save the model and vocabulary to a file.\n    \"\"\"\n    torch.save({\n        'model_state_dict': model.state_dict(),\n        'vocab': vocab\n    }, filepath)\n    print(f\"Model saved to {filepath}\")\ndef load_model(filepath, device):\n    \"\"\"\n    Load the model and vocabulary from a file.\n    \"\"\"\n    checkpoint = torch.load(filepath, map_location=device)\n    vocab = checkpoint['vocab']\n    input_size = len(vocab)\n    output_size = len(vocab)\n    embedding_dim = 256  # Make sure this matches your original model\n    hidden_size = 512    # Make sure this matches your original model\n    encoder = Encoder(input_size, embedding_dim, hidden_size).to(device)\n    decoder = Decoder(output_size, embedding_dim, hidden_size).to(device)\n    model = Seq2Seq(encoder, decoder, device).to(device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    print(f\"Model loaded from {filepath}\")\n    return model, vocab\ndef main():\n    # Load data from CSV\n    csv_file = '/kaggle/input/final-dataset/final_dataset.csv'\n    questions, haikus = load_data_from_csv(csv_file)\n    # Build vocabulary\n    all_texts = np.concatenate([questions, haikus])\n    vocab = build_vocab(all_texts)\n    idx_to_word = {idx: word for word, idx in vocab.items()}\n    # Hyperparameters\n    max_len = 50\n    embedding_dim = 256\n    hidden_size = 512\n    input_size = len(vocab)\n    output_size = len(vocab)\n    batch_size = 64\n    num_epochs = 50\n    learning_rate = 0.001\n    # Tokenize and pad questions and haikus\n    tokenized_questions = tokenize_and_pad(questions, vocab, max_len)\n    tokenized_haikus = tokenize_and_pad(haikus, vocab, max_len)\n    # Split the dataset into training and validation\n    train_questions, val_questions, train_haikus, val_haikus = train_test_split(tokenized_questions, tokenized_haikus, test_size=0.1)\n    # Dataset and DataLoader\n    train_dataset = HaikuDataset(train_questions, train_haikus)\n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    # Initialize encoder, decoder, Seq2Seq model\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    encoder = Encoder(input_size, embedding_dim, hidden_size).to(device)\n    decoder = Decoder(output_size, embedding_dim, hidden_size).to(device)\n    model = Seq2Seq(encoder, decoder, device).to(device)\n    # Optimizer and loss function\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    criterion = nn.CrossEntropyLoss(ignore_index=vocab['<PAD>'])\n    # Train the model\n    train_model(model, train_dataloader, optimizer, criterion, num_epochs)\n    # Save the model\n    save_model(model, vocab, 'haiku_generator_model.pth')\n    # Test the model with an example\n    test_question = tokenize_and_pad([\"What is the sky\"], vocab, max_len)[0]\n    generated_haiku = generate_haiku(model, test_question, vocab, idx_to_word)\n    print(f\"Generated Haiku: {generated_haiku}\")\ndef use_saved_model(question):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    # Load the saved model\n    model, vocab = load_model('haiku_generator_model.pth', device)\n    idx_to_word = {idx: word for word, idx in vocab.items()}\n    # Tokenize and generate haiku\n    max_len = 50  # Make sure this matches your original setting\n    tokenized_question = tokenize_and_pad([question], vocab, max_len)[0]\n    generated_haiku = generate_haiku(model, tokenized_question, vocab, idx_to_word)\n    return generated_haiku\nif __name__ == \"__main__\":\n    main()\n    # Example of using the saved model\n    new_question = \"What is the meaning of life?\"\n    haiku = use_saved_model(new_question)\n    print(f\"Question: {new_question}\")\n    print(f\"Generated Haiku: {haiku}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch import nn\n# First, recreate the necessary model architecture\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_dim, hidden_size, num_layers=1):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(input_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n    def forward(self, x):\n        embedded = self.embedding(x)\n        _, (hidden, cell) = self.lstm(embedded)\n        return hidden, cell\nclass Decoder(nn.Module):\n    def __init__(self, output_size, embedding_dim, hidden_size, num_layers=1):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(output_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(1)\n        embedded = self.embedding(x)\n        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n        prediction = self.fc(output.squeeze(1))\n        return prediction, hidden, cell\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n    def forward(self, source, target, teacher_forcing_ratio=0.5):\n        batch_size = target.shape[0]\n        target_len = target.shape[1]\n        target_vocab_size = self.decoder.fc.out_features\n        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n        hidden, cell = self.encoder(source)\n        input = target[:, 0]\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(input, hidden, cell)\n            outputs[:, t] = output\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input = target[:, t] if teacher_force else top1\n        return outputs\n# Function to load the model\ndef load_model(filepath, device):\n    checkpoint = torch.load(filepath, map_location=device)\n    vocab = checkpoint['vocab']\n    input_size = len(vocab)\n    output_size = len(vocab)\n    embedding_dim = 256  # Make sure this matches your original model\n    hidden_size = 512    # Make sure this matches your original model\n    encoder = Encoder(input_size, embedding_dim, hidden_size).to(device)\n    decoder = Decoder(output_size, embedding_dim, hidden_size).to(device)\n    model = Seq2Seq(encoder, decoder, device).to(device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    print(f\"Model loaded from {filepath}\")\n    return model, vocab\n# Function to tokenize and pad input\ndef tokenize_and_pad(texts, vocab, max_len):\n    tokenized = [[vocab.get(word, vocab['<UNK>']) for word in text.split()] for text in texts]\n    padded = [seq + [vocab['<PAD>']] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len] for seq in tokenized]\n    return torch.tensor(padded, dtype=torch.long)\n# Function to generate haiku\ndef generate_haiku(model, question, vocab, idx_to_word, max_len=20):\n    model.eval()\n    with torch.no_grad():\n        question = question.unsqueeze(0).to(model.device)\n        hidden, cell = model.encoder(question)\n        input = torch.tensor([vocab['<START>']]).to(model.device)\n        haiku = []\n        for _ in range(max_len):\n            output, hidden, cell = model.decoder(input, hidden, cell)\n            top1 = output.argmax(1)\n            word = idx_to_word[top1.item()]\n            if word == '<END>':\n                break\n            haiku.append(word)\n            input = top1\n        return ' '.join(haiku)\n# Load the saved model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel, vocab = load_model('/kaggle/input/haikuge/haiku_generator_model.pth', device)\nidx_to_word = {idx: word for word, idx in vocab.items()}\n# Function to use the model\ndef generate_haiku_from_question(question):\n    max_len = 50  # Make sure this matches your original setting\n    tokenized_question = tokenize_and_pad([question], vocab, max_len)[0]\n    return generate_haiku(model, tokenized_question, vocab, idx_to_word)\n# Example usage\nquestion = \"What steps can be taken to ensure that AI systems are designed in a way that promotes fairness and inclusivity while avoiding the reinforcement of harmful stereotypes?\"\nhaiku = generate_haiku_from_question(question)\nprint(f\"Question: {question}\")\nprint(f\"Generated Haiku: {haiku}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}