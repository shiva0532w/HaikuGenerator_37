{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10263009,"sourceType":"datasetVersion","datasetId":6348968}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Upgrade Libraries to Ensure Compatibility\n!pip install --upgrade transformers huggingface_hub accelerate peft datasets\n\n# Step 2: Restart the Kernel\n# In Kaggle, you'll need to manually restart the kernel by clicking on the Runtime menu and selecting Restart and Run All.\n# Once the kernel is restarted, re-run this notebook from the beginning.\n\n# Step 3: Import Necessary Modules\ntry:\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n    from peft import LoraConfig, get_peft_model, TaskType\n    from datasets import load_dataset\n    from torch.utils.data import DataLoader\n    import torch\n\n    print(\"All libraries imported successfully!\")\n\nexcept ImportError as e:\n    print(f\"ImportError: {e}\")\n    print(\"Ensure all libraries are properly installed and compatible.\")\n\n# Step 4: Validate the Installation\ntry:\n    from huggingface_hub import HfApi\n\n    # Test API call to Hugging Face Hub\n    api = HfApi()\n    print(\"Hugging Face Hub API initialized successfully!\")\n\n    # Test importing tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n    model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n    print(\"Transformers library and model loaded successfully!\")\n\nexcept Exception as e:\n    print(f\"Error during validation: {e}\")\n\n# Step 5: Example Task to Verify Everything Works\ntry:\n    # Load a small dataset as an example\n    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n    print(f\"Dataset loaded. Number of samples: {len(dataset)}\")\n\n    # Tokenize a sample text\n    sample_text = \"Transformers are amazing!\"\n    tokens = tokenizer(sample_text, return_tensors=\"pt\")\n    print(f\"Tokenized text: {tokens}\")\n\n    # Generate a prediction using the model\n    outputs = model.generate(**tokens, max_length=20)\n    generated_text = tokenizer.decode(outputs[0])\n    print(f\"Generated text: {generated_text}\")\n\nexcept Exception as e:\n    print(f\"Error during task execution: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T03:13:23.880632Z","iopub.execute_input":"2024-12-23T03:13:23.880924Z","iopub.status.idle":"2024-12-23T03:13:29.887756Z","shell.execute_reply.started":"2024-12-23T03:13:23.880902Z","shell.execute_reply":"2024-12-23T03:13:29.886967Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.27.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.1+cu121)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nAll libraries imported successfully!\nHugging Face Hub API initialized successfully!\nTransformers library and model loaded successfully!\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Dataset loaded. Number of samples: 36718\nTokenized text: {'input_ids': tensor([[41762,   364,   389,  4998,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\nGenerated text: Transformers are amazing!\n\nThe first thing I noticed was that the colors were very different from\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom datasets import Dataset\nimport json\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    BitsAndBytesConfig\n)\nfrom peft import (\n    prepare_model_for_kbit_training,\n    LoraConfig,\n    get_peft_model\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-23T03:13:33.368602Z","iopub.execute_input":"2024-12-23T03:13:33.368905Z","iopub.status.idle":"2024-12-23T03:13:33.898840Z","shell.execute_reply.started":"2024-12-23T03:13:33.368884Z","shell.execute_reply":"2024-12-23T03:13:33.898153Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Load dataset from JSON file\ndef load_dataset(file_path):\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    \n    return Dataset.from_dict({\n        'Question': [item['Question'] for item in data],\n        'Haiku': [item['Haiku'] for item in data]\n    })\n\n# Replace with your JSON file path\ndataset = load_dataset(\"/kaggle/input/hhhhhhh/final_dataset.json\")\nprint(f\"Dataset size: {len(dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T03:13:37.725768Z","iopub.execute_input":"2024-12-23T03:13:37.726069Z","iopub.status.idle":"2024-12-23T03:13:37.812236Z","shell.execute_reply.started":"2024-12-23T03:13:37.726047Z","shell.execute_reply":"2024-12-23T03:13:37.811357Z"}},"outputs":[{"name":"stdout","text":"Dataset size: 9679\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from huggingface_hub import login\n\n# Replace with your actual token from https://huggingface.co/settings/tokens\nlogin(\"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\")#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","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T03:13:53.163719Z","iopub.execute_input":"2024-12-23T03:13:53.164008Z","iopub.status.idle":"2024-12-23T03:13:53.215910Z","shell.execute_reply.started":"2024-12-23T03:13:53.163988Z","shell.execute_reply":"2024-12-23T03:13:53.215243Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Ensure the transformers library is installed and up-to-date\n!pip install --upgrade transformers\n\n# Import AutoTokenizer\nfrom transformers import AutoTokenizer\n\n# Initialize the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    \"google/gemma-2b\",\n    trust_remote_code=True\n)\n\n# Set pad_token to eos_token\ntokenizer.pad_token = tokenizer.eos_token\n\nprint(\"Tokenizer loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T03:13:55.491192Z","iopub.execute_input":"2024-12-23T03:13:55.491502Z","iopub.status.idle":"2024-12-23T03:14:01.105359Z","shell.execute_reply.started":"2024-12-23T03:13:55.491479Z","shell.execute_reply":"2024-12-23T03:14:01.104358Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39791d52eee044f483421b830ad00d06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c887e421934e40a58c2386680381b272"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04328aaa64034417a95f1d78fa709939"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0467a0fbe1e4457e9dcc9f2af108a6ba"}},"metadata":{}},{"name":"stdout","text":"Tokenizer loaded successfully!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    \"google/gemma-2b\",\n    trust_remote_code=True\n)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T03:14:03.928643Z","iopub.execute_input":"2024-12-23T03:14:03.928983Z","iopub.status.idle":"2024-12-23T03:14:05.095035Z","shell.execute_reply.started":"2024-12-23T03:14:03.928953Z","shell.execute_reply":"2024-12-23T03:14:05.094332Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Step 1: Install bitsandbytes\n!pip install bitsandbytes\n\n# Step 2: Import necessary modules\nfrom transformers import BitsAndBytesConfig\nimport torch\n\n# Step 3: Define the configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\nprint(\"BitsAndBytesConfig initialized successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T03:14:08.791417Z","iopub.execute_input":"2024-12-23T03:14:08.791736Z","iopub.status.idle":"2024-12-23T03:14:14.729574Z","shell.execute_reply.started":"2024-12-23T03:14:08.791707Z","shell.execute_reply":"2024-12-23T03:14:14.728682Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.1+cu121)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.0\nBitsAndBytesConfig initialized successfully!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T03:14:23.013566Z","iopub.execute_input":"2024-12-23T03:14:23.013887Z","iopub.status.idle":"2024-12-23T03:14:23.018945Z","shell.execute_reply.started":"2024-12-23T03:14:23.013858Z","shell.execute_reply":"2024-12-23T03:14:23.018179Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())  # Should return True\nprint(torch.cuda.device_count())  # Should return the number of GPUs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T03:14:26.407185Z","iopub.execute_input":"2024-12-23T03:14:26.407538Z","iopub.status.idle":"2024-12-23T03:14:26.518840Z","shell.execute_reply.started":"2024-12-23T03:14:26.407512Z","shell.execute_reply":"2024-12-23T03:14:26.518111Z"}},"outputs":[{"name":"stdout","text":"True\n2\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Step 1: Upgrade and install required libraries\n!pip install --upgrade bitsandbytes transformers accelerate peft\n\n# Step 2: Import required libraries\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training\nimport torch\n\n# Step 3: Verify installations and GPU compatibility\nprint(\"BitsAndBytes version:\", __import__('bitsandbytes').__version__)\nprint(\"Transformers version:\", __import__('transformers').__version__)\nprint(\"Torch CUDA available:\", torch.cuda.is_available())\nif not torch.cuda.is_available():\n    print(\"Warning: GPU is not available. Ensure GPU is enabled for optimal performance.\")\n\n# Step 4: Define the BitsAndBytesConfig for 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,                       # Enable 4-bit quantization\n    bnb_4bit_quant_type=\"nf4\",               # Use normal float 4 (nf4)\n    bnb_4bit_compute_dtype=torch.float16,    # Compute dtype\n    bnb_4bit_use_double_quant=True           # Use double quantization\n)\n\n# Step 5: Load the model with quantization configuration\ntry:\n    model = AutoModelForCausalLM.from_pretrained(\n        \"google/gemma-2b\",               # Replace with your model name\n        quantization_config=bnb_config, # Apply the 4-bit quantization config\n        device_map=\"auto\",              # Automatically map model to devices\n        trust_remote_code=True          # Allow using custom model code\n    )\n    print(\"Model loaded successfully with 4-bit quantization!\")\nexcept Exception as e:\n    print(f\"Error loading model: {e}\")\n\n# Step 6: Prepare the model for k-bit training\ntry:\n    model = prepare_model_for_kbit_training(model)\n    print(\"Model prepared for k-bit training!\")\nexcept Exception as e:\n    print(f\"Error preparing model for k-bit training: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T03:14:29.283369Z","iopub.execute_input":"2024-12-23T03:14:29.283679Z","iopub.status.idle":"2024-12-23T03:14:33.265985Z","shell.execute_reply.started":"2024-12-23T03:14:29.283655Z","shell.execute_reply":"2024-12-23T03:14:33.265184Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.1+cu121)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\nBitsAndBytes version: 0.45.0\nTransformers version: 4.47.1\nTorch CUDA available: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9dc6bfea47042ceb0fd531bc6a8e604"}},"metadata":{}},{"name":"stdout","text":"Error loading model: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\nModel prepared for k-bit training!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Step 1: Inspect the model to find valid target modules\nfor name, module in model.named_modules():\n    print(name)\n\n# Example output (you will see the actual layer names in your model):\n# ...\n# transformer.h.0.attention.query_layer\n# transformer.h.0.attention.key_layer\n# transformer.h.0.attention.value_layer\n# ...\n\n# Step 2: Update LoRA configuration with correct target module names\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    target_modules=[\"transformer.h.0.attention.query_layer\",  # Replace with actual names\n                    \"transformer.h.0.attention.key_layer\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Step 3: Apply LoRA to the model\nfrom peft import get_peft_model\n\ntry:\n    model = get_peft_model(model, lora_config)\n    model.print_trainable_parameters()  # Print trainable parameters info\n    print(\"LoRA applied successfully!\")\nexcept ValueError as e:\n    print(f\"Error applying LoRA: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T03:14:37.212420Z","iopub.execute_input":"2024-12-23T03:14:37.212736Z","iopub.status.idle":"2024-12-23T03:14:37.242997Z","shell.execute_reply.started":"2024-12-23T03:14:37.212712Z","shell.execute_reply":"2024-12-23T03:14:37.242342Z"}},"outputs":[{"name":"stdout","text":"\ntransformer\ntransformer.wte\ntransformer.wpe\ntransformer.drop\ntransformer.h\ntransformer.h.0\ntransformer.h.0.ln_1\ntransformer.h.0.attn\ntransformer.h.0.attn.c_attn\ntransformer.h.0.attn.c_proj\ntransformer.h.0.attn.attn_dropout\ntransformer.h.0.attn.resid_dropout\ntransformer.h.0.ln_2\ntransformer.h.0.mlp\ntransformer.h.0.mlp.c_fc\ntransformer.h.0.mlp.c_proj\ntransformer.h.0.mlp.act\ntransformer.h.0.mlp.dropout\ntransformer.h.1\ntransformer.h.1.ln_1\ntransformer.h.1.attn\ntransformer.h.1.attn.c_attn\ntransformer.h.1.attn.c_proj\ntransformer.h.1.attn.attn_dropout\ntransformer.h.1.attn.resid_dropout\ntransformer.h.1.ln_2\ntransformer.h.1.mlp\ntransformer.h.1.mlp.c_fc\ntransformer.h.1.mlp.c_proj\ntransformer.h.1.mlp.act\ntransformer.h.1.mlp.dropout\ntransformer.h.2\ntransformer.h.2.ln_1\ntransformer.h.2.attn\ntransformer.h.2.attn.c_attn\ntransformer.h.2.attn.c_proj\ntransformer.h.2.attn.attn_dropout\ntransformer.h.2.attn.resid_dropout\ntransformer.h.2.ln_2\ntransformer.h.2.mlp\ntransformer.h.2.mlp.c_fc\ntransformer.h.2.mlp.c_proj\ntransformer.h.2.mlp.act\ntransformer.h.2.mlp.dropout\ntransformer.h.3\ntransformer.h.3.ln_1\ntransformer.h.3.attn\ntransformer.h.3.attn.c_attn\ntransformer.h.3.attn.c_proj\ntransformer.h.3.attn.attn_dropout\ntransformer.h.3.attn.resid_dropout\ntransformer.h.3.ln_2\ntransformer.h.3.mlp\ntransformer.h.3.mlp.c_fc\ntransformer.h.3.mlp.c_proj\ntransformer.h.3.mlp.act\ntransformer.h.3.mlp.dropout\ntransformer.h.4\ntransformer.h.4.ln_1\ntransformer.h.4.attn\ntransformer.h.4.attn.c_attn\ntransformer.h.4.attn.c_proj\ntransformer.h.4.attn.attn_dropout\ntransformer.h.4.attn.resid_dropout\ntransformer.h.4.ln_2\ntransformer.h.4.mlp\ntransformer.h.4.mlp.c_fc\ntransformer.h.4.mlp.c_proj\ntransformer.h.4.mlp.act\ntransformer.h.4.mlp.dropout\ntransformer.h.5\ntransformer.h.5.ln_1\ntransformer.h.5.attn\ntransformer.h.5.attn.c_attn\ntransformer.h.5.attn.c_proj\ntransformer.h.5.attn.attn_dropout\ntransformer.h.5.attn.resid_dropout\ntransformer.h.5.ln_2\ntransformer.h.5.mlp\ntransformer.h.5.mlp.c_fc\ntransformer.h.5.mlp.c_proj\ntransformer.h.5.mlp.act\ntransformer.h.5.mlp.dropout\ntransformer.h.6\ntransformer.h.6.ln_1\ntransformer.h.6.attn\ntransformer.h.6.attn.c_attn\ntransformer.h.6.attn.c_proj\ntransformer.h.6.attn.attn_dropout\ntransformer.h.6.attn.resid_dropout\ntransformer.h.6.ln_2\ntransformer.h.6.mlp\ntransformer.h.6.mlp.c_fc\ntransformer.h.6.mlp.c_proj\ntransformer.h.6.mlp.act\ntransformer.h.6.mlp.dropout\ntransformer.h.7\ntransformer.h.7.ln_1\ntransformer.h.7.attn\ntransformer.h.7.attn.c_attn\ntransformer.h.7.attn.c_proj\ntransformer.h.7.attn.attn_dropout\ntransformer.h.7.attn.resid_dropout\ntransformer.h.7.ln_2\ntransformer.h.7.mlp\ntransformer.h.7.mlp.c_fc\ntransformer.h.7.mlp.c_proj\ntransformer.h.7.mlp.act\ntransformer.h.7.mlp.dropout\ntransformer.h.8\ntransformer.h.8.ln_1\ntransformer.h.8.attn\ntransformer.h.8.attn.c_attn\ntransformer.h.8.attn.c_proj\ntransformer.h.8.attn.attn_dropout\ntransformer.h.8.attn.resid_dropout\ntransformer.h.8.ln_2\ntransformer.h.8.mlp\ntransformer.h.8.mlp.c_fc\ntransformer.h.8.mlp.c_proj\ntransformer.h.8.mlp.act\ntransformer.h.8.mlp.dropout\ntransformer.h.9\ntransformer.h.9.ln_1\ntransformer.h.9.attn\ntransformer.h.9.attn.c_attn\ntransformer.h.9.attn.c_proj\ntransformer.h.9.attn.attn_dropout\ntransformer.h.9.attn.resid_dropout\ntransformer.h.9.ln_2\ntransformer.h.9.mlp\ntransformer.h.9.mlp.c_fc\ntransformer.h.9.mlp.c_proj\ntransformer.h.9.mlp.act\ntransformer.h.9.mlp.dropout\ntransformer.h.10\ntransformer.h.10.ln_1\ntransformer.h.10.attn\ntransformer.h.10.attn.c_attn\ntransformer.h.10.attn.c_proj\ntransformer.h.10.attn.attn_dropout\ntransformer.h.10.attn.resid_dropout\ntransformer.h.10.ln_2\ntransformer.h.10.mlp\ntransformer.h.10.mlp.c_fc\ntransformer.h.10.mlp.c_proj\ntransformer.h.10.mlp.act\ntransformer.h.10.mlp.dropout\ntransformer.h.11\ntransformer.h.11.ln_1\ntransformer.h.11.attn\ntransformer.h.11.attn.c_attn\ntransformer.h.11.attn.c_proj\ntransformer.h.11.attn.attn_dropout\ntransformer.h.11.attn.resid_dropout\ntransformer.h.11.ln_2\ntransformer.h.11.mlp\ntransformer.h.11.mlp.c_fc\ntransformer.h.11.mlp.c_proj\ntransformer.h.11.mlp.act\ntransformer.h.11.mlp.dropout\ntransformer.ln_f\nlm_head\nError applying LoRA: Target modules {'transformer.h.0.attention.query_layer', 'transformer.h.0.attention.key_layer'} not found in the base model. Please check the target modules and try again.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"def prepare_prompts(example):\n    return f\"Question: {example['Question']}\\nHaiku: {example['Haiku']}\"\n\ndef tokenize_function(examples, tokenizer):\n    prompts = [prepare_prompts({\"Question\": q, \"Haiku\": h}) \n               for q, h in zip(examples['Question'], examples['Haiku'])]\n    return tokenizer(\n        prompts,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n        return_tensors=\"pt\"\n    )\n\n# Tokenize the dataset\ntokenized_dataset = dataset.map(\n    lambda x: tokenize_function(x, tokenizer),\n    batched=True,\n    remove_columns=dataset.column_names\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T03:14:45.016563Z","iopub.execute_input":"2024-12-23T03:14:45.016881Z","iopub.status.idle":"2024-12-23T03:14:48.491247Z","shell.execute_reply.started":"2024-12-23T03:14:45.016854Z","shell.execute_reply":"2024-12-23T03:14:48.490554Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9679 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa92ff9096e84a429267d5269e313a7f"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"gemma-haiku-model\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    logging_steps=10,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=True,\n    warmup_steps=25,\n    save_strategy=\"epoch\",\n    evaluation_strategy=\"epoch\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T03:14:51.846831Z","iopub.execute_input":"2024-12-23T03:14:51.847146Z","iopub.status.idle":"2024-12-23T03:14:51.884326Z","shell.execute_reply.started":"2024-12-23T03:14:51.847122Z","shell.execute_reply":"2024-12-23T03:14:51.883490Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from transformers import TrainingArguments\n\n# Update or define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",      # Directory to save results\n    evaluation_strategy=\"no\",   # Disable evaluation\n    per_device_train_batch_size=8,\n    num_train_epochs=1,\n    save_steps=10_000,\n    save_total_limit=2,\n    logging_dir=\"./logs\",\n    logging_steps=500,\n    learning_rate=5e-5\n)\n\n# Initialize Trainer with the updated training arguments\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=lambda data: {\n        'input_ids': torch.stack([f['input_ids'] for f in data]),\n        'attention_mask': torch.stack([f['attention_mask'] for f in data])\n    }\n)\n\n# Start training\ntrainer.train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\nfrom datasets import load_dataset\n\n# Load dataset (example: using the \"glue\" dataset, replace with your own dataset)\ndataset = load_dataset(\"glue\", \"mrpc\")  # Example dataset, replace as needed\ntrain_dataset = dataset[\"train\"]\nval_dataset = dataset[\"validation\"]\n\n# Load pre-trained model and tokenizer\nmodel_name = \"bert-base-uncased\"  # Example model, replace as needed\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # num_labels=2 for binary classification\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Tokenize the dataset\ndef tokenize_function(examples):\n    return tokenizer(examples['sentence1'], examples['sentence2'], padding=\"max_length\", truncation=True)\n\ntokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\ntokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",         # Directory to save results\n    evaluation_strategy=\"no\",       # Disable evaluation\n    per_device_train_batch_size=8,  # Batch size per device\n    num_train_epochs=1,             # Number of training epochs\n    save_steps=10_000,              # Save model every 10,000 steps\n    save_total_limit=2,             # Keep only 2 saved models\n    logging_dir=\"./logs\",           # Directory for logs\n    logging_steps=500,              # Log every 500 steps\n    learning_rate=5e-5              # Learning rate\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_val_dataset,  # Optionally include eval dataset\n    data_collator=lambda data: {\n        'input_ids': torch.stack([f['input_ids'] for f in data]),\n        'attention_mask': torch.stack([f['attention_mask'] for f in data]),\n        'labels': torch.stack([f['label'] for f in data])  # Ensure labels are included\n    }\n)\n\n# Start training\ntrainer.train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the model\ntrainer.save_model(\"gemma-haiku-final\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to generate a haiku\ndef generate_haiku(question, model, tokenizer):\n    prompt = f\"Question: {question}\\nHaiku:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    outputs = model.generate(\n        **inputs,\n        max_length=100,\n        num_return_sequences=1,\n        temperature=0.7,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    \n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Test with a question\ntest_question = \"What is the sound of falling leaves?\"\ngenerated_haiku = generate_haiku(test_question, model, tokenizer)\nprint(f\"Question: {test_question}\")\nprint(f\"Generated Haiku: {generated_haiku}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}